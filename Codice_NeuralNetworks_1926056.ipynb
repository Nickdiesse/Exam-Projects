{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Progetto_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3lnFE5WNx3c"
      },
      "source": [
        "# Neural Network's Project: Causal Explanations of Image Misclassifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOgVJldqOPSz"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ70o95Q9dOW"
      },
      "source": [
        "# %tensorflow_version 1.x\n",
        "!pip install keras==2.3.1\n",
        "!pip install keras-metrics\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import keras_metrics as km\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
        "                         Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import applications\n",
        "from keras.models import Model, Input\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import sklearn.metrics \n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, GridSearchCV\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "print('Tensorflow ', tf.__version__)\n",
        "print('Keras ', keras.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "print(\"Libraries imported\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVfinvqZP6mr"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BvmsBozQJ3N"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import files \n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-omRuy-XKeA"
      },
      "source": [
        "SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG44jL4nXVaQ"
      },
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train, y_test = y_train.flatten(), y_test.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhZOmGACXZdf"
      },
      "source": [
        "\n",
        "print(\"x_train.shape:\", x_train.shape)\n",
        "print(\"y_train.shape\", y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TywXCB_tXb3f"
      },
      "source": [
        "K = len(set(y_train))\n",
        "print(\"number of classes:\", K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9E_1PfPKs22"
      },
      "source": [
        "# CNN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P283cTp4W9G"
      },
      "source": [
        "input_shape = (x_train[0].shape)\n",
        "n_classes = 10 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSMOyWJ7wPEe"
      },
      "source": [
        "NICKNET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DTCwfVg3jRX"
      },
      "source": [
        "def Nicknet(input_shape,n_classes):\n",
        "    regl2 = 0.0001\n",
        "    lr = 0.0001\n",
        "    model = Sequential()\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=96, input_shape=input_shape, kernel_size=(15,15),\\\n",
        "                     strides=(2,4), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=256, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "  \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=384, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "   \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Conv2D(filters=384, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=256, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Flatten())\n",
        "\n",
        "    flatten_shape = (input_shape[0]*input_shape[1]*input_shape[2],)\n",
        "    \n",
        "    \n",
        "    model.add(Dense(4096, input_shape=flatten_shape, kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(4096, kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(1000,kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(n_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    \n",
        "\n",
        "    adam = optimizers.Adam(lr=lr)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy',km.precision(),km.recall()])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = Nicknet(input_shape,n_classes) \n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBaE_PrHqBe9"
      },
      "source": [
        "NickNet 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-386Ah5kqKCN"
      },
      "source": [
        "def Nicknet2(input_shape,n_classes):\n",
        "    regl2 = 0.0001\n",
        "    lr = 0.0001\n",
        "    model = Sequential()\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=96, input_shape=input_shape, kernel_size=(15,15),\\\n",
        "                     strides=(2,4), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=256, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "  \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=384, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "   \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Conv2D(filters=384, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=256, kernel_size=(15,15), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Flatten())\n",
        "\n",
        "    flatten_shape = (input_shape[0]*input_shape[1]*input_shape[2],)\n",
        "    \n",
        "    \n",
        "    model.add(Dense(4096, input_shape=flatten_shape, kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(4096, kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(1000,kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(n_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    \n",
        "\n",
        "    sgd = optimizers.SGD(lr=lr)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy',km.precision(),km.recall()])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = Nicknet2(input_shape,n_classes) \n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEIwZ_IqLrM"
      },
      "source": [
        "NickNet3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLN1kK4yqN4B"
      },
      "source": [
        "def Nicknet3(input_shape,n_classes):\n",
        "    regl2 = 0.0001\n",
        "    lr = 0.0001\n",
        "    model = Sequential()\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=96, input_shape=input_shape, kernel_size=(10,10),\\\n",
        "                     strides=(2,4), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=256, kernel_size=(10,10), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "  \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=384, kernel_size=(10,10), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "   \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Conv2D(filters=384, kernel_size=(10,10), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "   \n",
        "    model.add(Conv2D(filters=256, kernel_size=(10,10), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Flatten())\n",
        "\n",
        "    flatten_shape = (input_shape[0]*input_shape[1]*input_shape[2],)\n",
        "    \n",
        "    \n",
        "    model.add(Dense(4096, input_shape=flatten_shape, kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(4096, kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(1000,kernel_regularizer=regularizers.l2(regl2)))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dropout(0.4))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    \n",
        "    model.add(Dense(n_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    \n",
        "\n",
        "    sgd = optimizers.SGD(lr=lr)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy',km.precision(),km.recall()])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = Nicknet3(input_shape,n_classes) \n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf5p6Aa4LDhb"
      },
      "source": [
        "# Training \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5gWaJr_LGha"
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 30\n",
        "\n",
        "\n",
        "history = model.fit(x_train,y_train, batch_size = batch_size , epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test,y_test),\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYOn-z4dNPXC"
      },
      "source": [
        "# METRICS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVulj8vQKFbh"
      },
      "source": [
        "CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9RN1Kw8rU4e"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "plt.rcParams['figure.figsize'] = [10,7]\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "  if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "\n",
        "  print(cm)\n",
        "\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=45)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  fmt = '.2f' if normalize else 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, format(cm[i, j], fmt),\n",
        "               horizontalalignment=\"center\",\n",
        "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "p_test = model.predict(x_test).argmax(axis=1)\n",
        "cm = confusion_matrix(y_test, p_test)\n",
        "plot_confusion_matrix(cm, list(range(10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOoE5k-89-Ko"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHNWzhQjXRQD"
      },
      "source": [
        "plt.plot(history.history['precision'])\n",
        "plt.plot(history.history['val_precision'])\n",
        "plt.title('model precision')\n",
        "plt.ylabel('precision')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBJ_wTV6Hp4H"
      },
      "source": [
        "# DEFINE THE LABELS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XCQIIIAHsuK"
      },
      "source": [
        "labels = '''airplane\n",
        "automobile\n",
        "bird\n",
        "cat\n",
        "deer\n",
        "dog\n",
        "frog\n",
        "horse\n",
        "ship\n",
        "truck'''.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV8rFgY-HyJ5"
      },
      "source": [
        "Check The right predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X8BfuwqHuS3"
      },
      "source": [
        "misclassified_idx = np.where(p_test == y_test)[0]\n",
        "i = np.random.choice(misclassified_idx)\n",
        "plt.imshow(x_test[i], cmap='gray')\n",
        "plt.title(\"True label: %s Predicted: %s\" % (labels[y_test[i]], labels[p_test[i]]));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvivS-mmH42H"
      },
      "source": [
        "Check the wrong predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYS21gpQH4bZ"
      },
      "source": [
        "misclassified_idx = np.where(p_test != y_test)[0]\n",
        "i = np.random.choice(misclassified_idx)\n",
        "plt.imshow(x_test[i], cmap='gray')\n",
        "plt.title(\"True label: %s Predicted: %s\" % (labels[y_test[i]], labels[p_test[i]]));"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
